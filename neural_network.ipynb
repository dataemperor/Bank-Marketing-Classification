{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"bank-additional\\\\bank-additional\\\\bank-additional-full.csv\"\n",
    "data = pd.read_csv(filePath, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n",
    "Transforming non-numeric labels into numeric labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_label_encoder = LabelEncoder()\n",
    "\n",
    "for variable in data.select_dtypes(include=[\"object\"]).columns:\n",
    "    data[variable] = categorical_label_encoder.fit_transform(data[variable])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating features and target\n",
    "Seperating/Dropping the target variable (AKA the output variable) from the features (input variables). This is done because the model the target variable itself isn't used to train the model, instead the features are to train the model in order to predict the target variable; therefore, to prevent the model from learning incorrect patterns and because the model is evaluated on the target variable, it is necessary to drop the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(\"y\", axis=1)\n",
    "target = data[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "The aim of Standardization is to increase data quality through eliminating errors, inconsistencies and redundant data. Through having a higher quality dataset, the model will be able to learn patterns from more verbose data leading to better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "scaled_features = standard_scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "A dataset is split into two parts, the training dataset and the testing dataset. This done so a model can be tested/evaluated after training it, without testing the model, it won't be possbile to find out whether a it is underfitted or overfitted (Bias-Variance Tradeoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network Model\n",
    "\n",
    "This model will be using a Multilayer Perceptron with an input layer, a hidden layer and an output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_features = features.shape[1]\n",
    "number_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding the Number of Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding the Number of Neurons in the Input Layer and the Output Layer\n",
    "Since there are 20 inputs, there will be 20 neurons in the input layer. Since an instance can only be classified into one of two types, this model is using **Binary Classification**, therefore it will be using one neuron for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deciding the Number of Neurons in the Hidden Layer/Layers\n",
    "However, deciding the number of Neurons in the hidden layers, or in this case, the hidden layer is not as straightforward as the other layers. The complexity of the dataset (dimensionality, sample size, noise, distribution, etc) and whether the model is underfit or overfit has an effect on how many neurons should be in the hidden layer.\n",
    "\n",
    "According to [The Number of Hidden Layers](https://www.heatonresearch.com/2017/06/01/hidden-layers.html), there are a few rules of thumb for determining the number of neurons in a hidden layer (Additionally, this article discusses about Deep Learning and why multiple hidden layers are used despite the Universal Approximation Theorem proving that a single hidden layer neural network can learn anything). \n",
    "\n",
    "1. The average of the sum of the number of Input and Output Neurons.\n",
    "2. Sum of 2/3rds of the number of Input Neurons and the number of Output Neurons.\n",
    "3. Less than two times the number of Input Neurons\n",
    "\n",
    "The first heuristic tends to give a smaller number of hidden neurons, since there are benefits to a smaller networks such as a faster training time and being less prone to overfitting, therefore we will be deciding hte number of hidden neurons by getting the average of the sum of of the number of Input and Output Neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_input_neurons = number_of_features\n",
    "number_of_output_neurons = 1\n",
    "number_of_hidden_neurons = math.ceil((number_of_input_neurons + number_of_output_neurons) / 2)\n",
    "number_of_hidden_neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding the Activation Function\n",
    "For the activation function, ReLU will be due to the fact that it is simple and efficient, it is also the standard activation function that is used for classification tasks, with other functions such as sigmoid or Tanh being used for situations where ReLU isn't optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mail2\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_layer = Dense(number_of_input_neurons, input_shape=(number_of_features,))\n",
    "hidden_layer = Dense(number_of_hidden_neurons, activation=\"relu\")\n",
    "output_layer = Dense(number_of_output_neurons)\n",
    "\n",
    "neural_network_model = Sequential([input_layer, hidden_layer, output_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "An Optimizer modifies the weights during training to reduce the difference between predicted values and actual values, also known as **'Loss'**. There are many optimizers such as Gradient Descent, Stochastic Gradient Descent, RMSprop, Adam, etc. Optimizers are divided into two groups according to how the **learning rate** is modified. If you have to manually tune the learning rate, it is a **Gradient Descent Algorithm** and if it can automatically adapt, it is a **Adaptive Algorithm**. This model will be using an Adaptive Optimizer called **'Adam'** due to its ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function\n",
    "A Loss Function is used to measure how well a model can predict values, this is done by taking the difference between a predicted value and its corresponding actual value, the difference between these two are called **'Loss'**. This model will be using **Binary Cross Entropy** also known as **Logarithmic Loss** or **Log Loss**, this loss function is being used because it makes the model more likely to have output values close to 0 or 1 and because it is standard for binary classification.\n",
    "\n",
    "The aforementioned Optimizer uses the loss from a loss function to modify the weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Neural Network Model\n",
    "### Batch Size\n",
    "The Batch Size is a hyperparameter that defines the **number of samples/datapoints** to work through before updating the model's internal parameters as weights, biases, etc.\n",
    "\n",
    "Batches are used because updating the parameters on the entire training dataset is computationally expensive (unless the dataset is small) and because it leads to a shorter training time. However having a large batch size might mean that the model is less accurate and prone to overfitting.\n",
    "\n",
    "### Number of Epochs\n",
    "The Number of Epochs is a hyperparameter that defines the **number of training iterations** for the model.\n",
    "\n",
    "As the model goes through each epoch, its parameters are updated and patterns in the dataset are learned. It is important to have a balanced amount of epochs as too few leads to underfitting and the inability of the model to extract patterns from the dataset and on the other hand, having too many epochs leads to the model being overfit, which leads to poor predictions on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8245 - loss: 1.8723 - val_accuracy: 0.8657 - val_loss: 1.4038\n",
      "Epoch 2/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8739 - loss: 1.2037 - val_accuracy: 0.8829 - val_loss: 0.7788\n",
      "Epoch 3/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8851 - loss: 0.7772 - val_accuracy: 0.8876 - val_loss: 0.8058\n",
      "Epoch 4/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.6879 - val_accuracy: 0.8958 - val_loss: 0.6110\n",
      "Epoch 5/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8940 - loss: 0.5477 - val_accuracy: 0.8935 - val_loss: 0.4951\n",
      "Epoch 6/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8920 - loss: 0.4824 - val_accuracy: 0.8888 - val_loss: 0.4605\n",
      "Epoch 7/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8950 - loss: 0.4564 - val_accuracy: 0.8689 - val_loss: 0.5587\n",
      "Epoch 8/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8788 - loss: 0.5115 - val_accuracy: 0.8924 - val_loss: 0.4059\n",
      "Epoch 9/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8965 - loss: 0.3854 - val_accuracy: 0.8936 - val_loss: 0.3908\n",
      "Epoch 10/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.3728 - val_accuracy: 0.8976 - val_loss: 0.3536\n",
      "Epoch 11/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9008 - loss: 0.3157 - val_accuracy: 0.8944 - val_loss: 0.3223\n",
      "Epoch 12/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8995 - loss: 0.2916 - val_accuracy: 0.8904 - val_loss: 0.5075\n",
      "Epoch 13/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.3455 - val_accuracy: 0.8983 - val_loss: 0.3667\n",
      "Epoch 14/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8980 - loss: 0.3420 - val_accuracy: 0.9011 - val_loss: 0.3188\n",
      "Epoch 15/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9016 - loss: 0.2784 - val_accuracy: 0.9014 - val_loss: 0.3020\n",
      "Epoch 16/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8972 - loss: 0.3209 - val_accuracy: 0.8936 - val_loss: 0.3410\n",
      "Epoch 17/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.2775 - val_accuracy: 0.9027 - val_loss: 0.3317\n",
      "Epoch 18/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2610 - val_accuracy: 0.9074 - val_loss: 0.2674\n",
      "Epoch 19/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9054 - loss: 0.2440 - val_accuracy: 0.9097 - val_loss: 0.2789\n",
      "Epoch 20/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8628 - loss: 0.5649 - val_accuracy: 0.9009 - val_loss: 0.3196\n",
      "Epoch 21/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8918 - loss: 0.3041 - val_accuracy: 0.8970 - val_loss: 0.2701\n",
      "Epoch 22/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8973 - loss: 0.2662 - val_accuracy: 0.8985 - val_loss: 0.2534\n",
      "Epoch 23/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8969 - loss: 0.2675 - val_accuracy: 0.9015 - val_loss: 0.2484\n",
      "Epoch 24/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8980 - loss: 0.2566 - val_accuracy: 0.8973 - val_loss: 0.2637\n",
      "Epoch 25/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8982 - loss: 0.2758 - val_accuracy: 0.9002 - val_loss: 0.2453\n",
      "Epoch 26/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9018 - loss: 0.2504 - val_accuracy: 0.9014 - val_loss: 0.2477\n",
      "Epoch 27/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9020 - loss: 0.2487 - val_accuracy: 0.8998 - val_loss: 0.2634\n",
      "Epoch 28/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9036 - loss: 0.2938 - val_accuracy: 0.9014 - val_loss: 0.2530\n",
      "Epoch 29/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9038 - loss: 0.2497 - val_accuracy: 0.9052 - val_loss: 0.2366\n",
      "Epoch 30/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9024 - loss: 0.2449 - val_accuracy: 0.9026 - val_loss: 0.2379\n",
      "Epoch 31/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9065 - loss: 0.2376 - val_accuracy: 0.9024 - val_loss: 0.2491\n",
      "Epoch 32/32\n",
      "\u001b[1m824/824\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9039 - loss: 0.2311 - val_accuracy: 0.9032 - val_loss: 0.2359\n"
     ]
    }
   ],
   "source": [
    "trained_neural_network = neural_network_model.fit(X_train, y_train, batch_size=32, epochs=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step\n",
      "0.9068948773974266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      7303\n",
      "           1       0.69      0.33      0.44       935\n",
      "\n",
      "    accuracy                           0.91      8238\n",
      "   macro avg       0.80      0.65      0.70      8238\n",
      "weighted avg       0.89      0.91      0.89      8238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = (neural_network_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
